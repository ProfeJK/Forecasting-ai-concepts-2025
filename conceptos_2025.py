# -*- coding: utf-8 -*-
"""Conceptos_2025.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lHANNlZAVpd0bu4Vmq7DqqFyAb8yLMYK

# ANALITICA DE DATOS CON INTELIGENCIA ARTIFICIAL

# Conceptos de estadistica y analitica

**Universidad de los Andes**

2025

---


Juan Carlos Vega, M.Sc.

# üìå Introducci√≥n a la selecci√≥n y extracci√≥n de caracter√≠sticas

En problemas de **aprendizaje autom√°tico y modelado predictivo**, no todas las variables disponibles aportan informaci√≥n √∫til.  
Algunas pueden ser **irrelevantes, redundantes o incluso introducir ruido**, lo cual afecta la capacidad del modelo para generalizar.  

Por eso existen dos enfoques fundamentales:

- **Selecci√≥n de caracter√≠sticas (Feature Selection):** consiste en **elegir un subconjunto** de las variables originales que mejoran el rendimiento del modelo sin perder informaci√≥n esencial.  
  Ejemplos: RFE (Recursive Feature Elimination), b√∫squeda exhaustiva, regularizaci√≥n con Lasso.

- **Extracci√≥n de caracter√≠sticas (Feature Extraction):** consiste en **transformar las variables originales** en nuevas representaciones m√°s compactas y √∫tiles, preservando la informaci√≥n m√°s relevante.  
  Ejemplos: PCA (An√°lisis de Componentes Principales), Autoencoders.

Estos m√©todos ayudan a:
- Reducir la **dimensionalidad** y el **costo computacional**.  
- Evitar el **overfitting**.  
- Mejorar la **interpretabilidad** del modelo.  
- Destacar la informaci√≥n m√°s relevante de los datos.

En este cap√≠tulo veremos t√©cnicas clave como **RFE, b√∫squeda exhaustiva, regresi√≥n Lasso y Ridge**, analizando c√≥mo funcionan, sus ventajas, limitaciones y cu√°ndo conviene aplicarlas.

## üîé Comparaci√≥n PCA vs LCA

| Caracter√≠stica            | **PCA (Principal Component Analysis)**                                      | **LCA (Latent Class Analysis)**                                       |
|----------------------------|----------------------------------------------------------------------------|------------------------------------------------------------------------|
| **Tipo de t√©cnica**        | Reducci√≥n de dimensionalidad (an√°lisis factorial, m√©todo lineal).          | Modelado probabil√≠stico (clasificaci√≥n no supervisada).                |
| **Objetivo principal**     | Encontrar combinaciones lineales de variables (componentes principales) que expliquen la mayor varianza. | Identificar **clases/categor√≠as ocultas** (no observables) en la poblaci√≥n. |
| **Datos de entrada**       | Variables num√©ricas continuas, preferiblemente correlacionadas.            | Variables categ√≥ricas (aunque tambi√©n se puede usar en datos mixtos).  |
| **Salida**                 | Nuevas variables continuas (componentes), ordenadas por importancia (varianza explicada). | Probabilidades de pertenencia de cada individuo a clases latentes.     |
| **Supuesto clave**         | Las relaciones importantes se capturan en la **varianza lineal** de los datos. | Los individuos pertenecen a un n√∫mero finito de **clases ocultas** mutuamente excluyentes. |
| **Interpretaci√≥n**         | Dif√≠cil: cada componente es combinaci√≥n de muchas variables.               | M√°s interpretativo: cada clase suele corresponder a un perfil o segmento de poblaci√≥n. |
| **Aplicaciones t√≠picas**   | - Reducci√≥n de dimensionalidad antes de ML.<br>- Compresi√≥n de im√°genes.<br>- An√°lisis de se√±ales o gen√≥mica.<br>- Eliminar colinealidad. | - Psicolog√≠a y ciencias sociales.<br>- Segmentaci√≥n de clientes.<br>- Educaci√≥n (perfiles de estudiantes).<br>- Epidemiolog√≠a (subpoblaciones). |
| **Cu√°ndo usarlo**          | Cuando tienes **muchas variables continuas** y quieres resumirlas en menos dimensiones sin perder demasiada informaci√≥n. | Cuando sospechas que existen **subgrupos ocultos** en tu muestra y quieres descubrirlos. |

**En resumen:**

**PCA ‚Üí** te da nuevas variables continuas (componentes) que resumen la informaci√≥n.

**LCA ‚Üí** te da clases ocultas (clusters probabil√≠sticos) a las que cada individuo pertenece con cierta probabilidad.

üëâ Si quieres reducir dimensionalidad de features, usa PCA.
üëâ Si quieres segmentar la poblaci√≥n en perfiles ocultos, usa LCA.

## üîé Comparaci√≥n de tipos de correlaci√≥n

| Tipo de correlaci√≥n       | Qu√© mide                                                                 | Datos apropiados                                   | Rango/Salida         | Supuestos principales                                        | Cu√°ndo usarla |
|---------------------------|--------------------------------------------------------------------------|---------------------------------------------------|----------------------|--------------------------------------------------------------|---------------|
| **Pearson (r)**           | Fuerza y direcci√≥n de la **relaci√≥n lineal** entre dos variables.        | Continuas, distribuidas aproximadamente normal.   | -1 a +1              | Linealidad, homocedasticidad, normalidad aproximada.         | Cuando esperas relaci√≥n lineal en datos continuos. |
| **Spearman (œÅ)**          | Relaci√≥n **mon√≥tona** (creciente o decreciente), basada en rangos.       | Continuas u ordinales (no requiere normalidad).   | -1 a +1              | Relaci√≥n mon√≥tona (no necesariamente lineal).                | Cuando hay outliers o relaci√≥n no lineal pero mon√≥tona. |
| **Kendall (œÑ)**           | Concordancia de rangos entre dos variables.                             | Ordinales o continuas con pocos datos.            | -1 a +1              | Similar a Spearman pero m√°s robusta en muestras peque√±as.    | Cuando la muestra es peque√±a o hay muchos empates en rangos. |
| **Phi (œÜ)**               | Asociaci√≥n entre **dos variables dicot√≥micas** (binarias).              | Categ√≥ricas binarias.                             | -1 a +1              | Tablas 2x2.                                                  | En an√°lisis de variables binarias (ej. s√≠/no). |
| **Cramer‚Äôs V**            | Asociaci√≥n entre variables **categ√≥ricas nominales** (m√°s de 2 clases). | Categ√≥ricas nominales.                            | 0 a 1                | Basado en chi-cuadrado.                                      | Para tablas de contingencia grandes (>2x2). |
| **Point-Biserial (r_pb)** | Correlaci√≥n entre una variable continua y una binaria.                  | Continua + binaria.                               | -1 a +1              | Variable binaria codificada (0/1).                          | En psicometr√≠a y pruebas dicot√≥micas vs puntuaciones. |

---

### ‚úÖ Resumen r√°pido:
- **Pearson**: datos continuos, relaci√≥n lineal.  
- **Spearman / Kendall**: datos ordinales o no normales, relaci√≥n mon√≥tona.  
- **Phi / Cramer‚Äôs V**: datos categ√≥ricos.  
- **Point-Biserial**: una continua + una binaria.

## üîé Correlaci√≥n vs. Correlaci√≥n Cruzada

| Concepto                  | Definici√≥n                                                                 | Uso principal                                                   | Ejemplo t√≠pico |
|---------------------------|-----------------------------------------------------------------------------|----------------------------------------------------------------|----------------|
| **Correlaci√≥n (cl√°sica)** | Mide el grado de relaci√≥n (lineal, mon√≥tona, ordinal) entre **dos variables** en un mismo instante o conjunto de observaciones. | Estad√≠stica descriptiva, psicometr√≠a, ciencias sociales.        | Relaci√≥n entre horas de estudio y calificaci√≥n en un examen. |
| **Correlaci√≥n cruzada**   | Extiende la correlaci√≥n considerando **desplazamientos temporales (lags)** entre dos series. Eval√∫a c√≥mo se relaciona una serie con otra en diferentes retardos. | Series temporales, se√±ales, econometr√≠a, climatolog√≠a.          | Ver si la temperatura de hoy predice la demanda el√©ctrica de ma√±ana. |

---

### üîπ Detalles clave de la **correlaci√≥n cruzada**
- Se calcula como:
  \[
  C_{xy}(k) = \sum_t x(t) \cdot y(t+k)
  \]
  donde `k` es el **desplazamiento temporal (lag)**.
- Si el valor m√°ximo ocurre en `k=+3`, significa que la serie `x` **se adelanta 3 pasos** respecto a `y`.
- Es muy usada para:
  - Identificar relaciones de causa/efecto temporal.
  - Alinear se√±ales (ej. en sismolog√≠a o procesamiento de audio).
  - Predecir variables con rezago (ej. series econ√≥micas).

---

### ‚úÖ Resumen r√°pido
- **Correlaci√≥n simple**: mide la relaci√≥n instant√°nea entre dos variables.  
- **Correlaci√≥n cruzada**: mide la relaci√≥n cuando una variable puede estar **desfasada en el tiempo** respecto a la otra.  

üëâ Usa correlaci√≥n simple si tus datos no tienen orden temporal.  
üëâ Usa correlaci√≥n cruzada si trabajas con **series temporales** o se√±ales y sospechas que la dependencia aparece con retraso.

## üîé Autocorrelaci√≥n

| Concepto              | Definici√≥n                                                                 | Uso principal                                              | Ejemplo t√≠pico |
|-----------------------|-----------------------------------------------------------------------------|-----------------------------------------------------------|----------------|
| **Autocorrelaci√≥n**   | Es la correlaci√≥n de una serie consigo misma en diferentes desplazamientos temporales (lags). | Detectar patrones repetitivos o dependencias temporales.  | Ver si la demanda el√©ctrica de hoy se parece a la de hace 24 horas. |

---

### üîπ Detalles clave
- Matem√°ticamente es un caso especial de la **correlaci√≥n cruzada** donde `x = y`.  
- Se calcula como:
  \[
  R_{xx}(k) = \sum_t x(t) \cdot x(t+k)
  \]
- √ötil para:
  - Identificar **estacionalidad** o periodicidad en series (ej. patrones diarios, semanales, anuales).
  - Ver cu√°nto dura la ‚Äúmemoria‚Äù de una serie (ej. procesos AR en estad√≠stica).
  - Diagn√≥stico en modelos de series temporales (ej. usar ACF y PACF en ARIMA).

---

### üîπ Relaci√≥n con otros conceptos
- **Correlaci√≥n simple**: compara dos variables al mismo tiempo.  
- **Correlaci√≥n cruzada**: compara dos series en diferentes retardos.  
- **Autocorrelaci√≥n**: compara **una sola serie consigo misma** en distintos retardos.

---

### ‚úÖ Resumen r√°pido
- Si quieres saber c√≥mo dos variables se relacionan ‚Üí usa **correlaci√≥n**.  
- Si quieres saber si una variable ‚Äúinfluye con retraso‚Äù en otra ‚Üí usa **correlaci√≥n cruzada**.  
- Si quieres saber si una variable depende de su propio pasado ‚Üí usa **autocorrelaci√≥n**.

## üîé Correlaci√≥n parcial

### üìå Definici√≥n
La **correlaci√≥n parcial** mide la relaci√≥n entre dos variables **controlando el efecto de una o m√°s variables adicionales**.  
Es decir, eval√∫a cu√°nto se relacionan dos variables de forma **directa**, eliminando la influencia de terceras variables que puedan estar introduciendo dependencia.

---

### üìå Diferencia con la correlaci√≥n simple
- **Correlaci√≥n simple**: mide la asociaci√≥n total entre X e Y (incluyendo efectos de otras variables).
- **Correlaci√≥n parcial**: mide solo la asociaci√≥n **residual**, despu√©s de eliminar el efecto de otras variables Z.

Ejemplo:  
Si medimos **ejercicio (X)** y **peso corporal (Y)**, puede que est√©n correlacionados.  
Pero al controlar por **ingesta cal√≥rica (Z)**, la correlaci√≥n parcial entre ejercicio y peso podr√≠a ser menor, mostrando que la dieta explica gran parte de la relaci√≥n.

---

### üìå En series temporales
En an√°lisis de series, la **funci√≥n de autocorrelaci√≥n parcial (PACF)** es fundamental:
- **ACF (Autocorrelaci√≥n)**: muestra la correlaci√≥n de una serie consigo misma en diferentes retardos, pero incluye efectos indirectos.
- **PACF (Autocorrelaci√≥n parcial)**: muestra la correlaci√≥n **directa** entre la serie y su retardo, eliminando la influencia de retardos intermedios.

üëâ Por eso, en modelos **AR(p)** se usa la PACF para identificar el orden `p`.

---

### üìå Interpretaci√≥n en PACF
- Si el PACF muestra un corte abrupto despu√©s del lag `p` ‚Üí sugiere un modelo **AR(p)**.  
- Si el PACF decae gradualmente ‚Üí el proceso puede ser m√°s complejo (ej. mixto ARMA).  

---

### ‚úÖ En resumen
- **Correlaci√≥n parcial**: mide la relaci√≥n directa entre dos variables controlando otras.  
- **PACF en series**: herramienta clave para seleccionar el n√∫mero de retardos en modelos AR/ARIMA.  
- Complementa a la **ACF**, que muestra las correlaciones totales (directas + indirectas).
"""

!pip install statsmodels

import numpy as np
import pandas as pd

# Semilla para reproducibilidad
rng = np.random.default_rng(42)

# Serie AR(2): y_t = 0.6 y_{t-1} - 0.3 y_{t-2} + ruido
n = 500
y = np.zeros(n)
noise = rng.normal(0, 1, size=n)
for t in range(2, n):
    y[t] = 0.6*y[t-1] - 0.3*y[t-2] + noise[t]

# Si ya tienes tu serie como pandas.Series, usa:
# y = tu_serie.values.astype(float)

"""## calculo numerico de ACF  y PACF

"""

from statsmodels.tsa.stattools import acf, pacf

# nlags: cu√°ntos retardos inspeccionar
nlags = 40

acf_vals = acf(y, nlags=nlags, fft=True)         # incluye lag 0
pacf_vals = pacf(y, nlags=nlags, method='yw')    # Yule-Walker para PACF

print("ACF (primeros 10 lags):", np.round(acf_vals[:10], 3))
print("PACF (primeros 10 lags):", np.round(pacf_vals[:10], 3))

import matplotlib.pyplot as plt
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf

# ACF AUTOCORRELACION
fig1 = plot_acf(y, lags=nlags, zero=False)
plt.title("ACF")
plt.show()

# PACF AUTOCORRELACION PARCIAL
fig2 = plot_pacf(y, lags=nlags, zero=False, method='yw')
plt.title("PACF")
plt.show()

# ejemplo de correlaci√≥n cruzada
import numpy as np

def xcorr_lag(x, y, max_lag=48):  # ajusta a tu frecuencia
    lags = np.arange(-max_lag, max_lag+1)
    corrs = []
    x = (x - x.mean())/x.std()
    y = (y - y.mean())/y.std()
    for k in lags:
        if k < 0:
            corrs.append(np.corrcoef(x[-k:], y[:len(y)+k])[0,1])
        elif k > 0:
            corrs.append(np.corrcoef(x[:len(x)-k], y[k:])[0,1])
        else:
            corrs.append(np.corrcoef(x, y)[0,1])
    idx = int(np.nanargmax(np.abs(corrs)))
    return lags[idx], corrs[idx], lags, np.array(corrs)

# Demo con la misma y + ruido desplazada
y2 = np.roll(y, 3) + 0.1*np.random.randn(len(y))  # y adelantada 3 pasos
best_lag, best_corr, lags, corrs = xcorr_lag(y, y2, max_lag=24)
print(f"Mejor lag: {best_lag}  |  correlaci√≥n: {best_corr:.3f}")

"""## üîé Descomposici√≥n y pruebas de estacionariedad en series temporales

### üìå Descomposici√≥n de series
La **descomposici√≥n** consiste en separar una serie temporal en sus componentes b√°sicos:

1. **Tendencia (T)** ‚Üí movimiento de largo plazo (creciente, decreciente o estable).  
2. **Estacionalidad (S)** ‚Üí patrones que se repiten en intervalos fijos (diario, semanal, anual).  
3. **Componente aleatoria o residual (R)** ‚Üí variaciones no explicadas (ruido).

Se expresa como:
- Modelo aditivo:  
  \[
  Y_t = T_t + S_t + R_t
  \]
- Modelo multiplicativo:  
  \[
  Y_t = T_t \times S_t \times R_t
  \]

üëâ Sirve para **entender la estructura** de la serie y elegir mejor el modelo de predicci√≥n.

---

### üìå Estacionariedad
Una serie es **estacionaria** cuando sus propiedades estad√≠sticas **no cambian en el tiempo**:
- Media constante.  
- Varianza constante.  
- Autocorrelaci√≥n estable en todos los periodos.

Por qu√© importa:
- Modelos cl√°sicos como **AR, MA, ARIMA** requieren estacionariedad para funcionar correctamente.  
- La estacionariedad permite que las relaciones pasadas sean v√°lidas para el futuro.

---

### üìå Pruebas de estacionariedad
Las m√°s usadas son:
- **ADF (Augmented Dickey-Fuller)**: H0 = la serie tiene ra√≠z unitaria (no es estacionaria).  
- **KPSS (Kwiatkowski-Phillips-Schmidt-Shin)**: H0 = la serie es estacionaria.  
- **Phillips-Perron**: variante robusta para correlaci√≥n serial y heterocedasticidad.

üëâ Lo ideal es usar m√°s de una prueba para confirmar.

---

### üìå Relaci√≥n entre ambos pasos
- La **descomposici√≥n** te da una idea visual de qu√© parte de la serie se debe eliminar o modelar (tendencia/estacionalidad).  
- Las **pruebas de estacionariedad** te dicen si despu√©s de quitar esas partes (o aplicar transformaciones como diferenciaci√≥n) tu serie queda lista para modelarse.

---

### ‚úÖ En resumen
- **Descomposici√≥n** ‚Üí entender la estructura (tendencia, estacionalidad, ruido).  
- **Pruebas de estacionariedad** ‚Üí confirmar si la serie puede ser modelada directamente o requiere transformaciones (ej. diferenciaci√≥n).  
- Ambos pasos son la base para decidir entre usar modelos **estad√≠sticos (ARIMA/ETS)** o m√°s flexibles como **LSTM/Transformers**.

# üéØ Selecci√≥n y extracci√≥n de caracter√≠sticas (Feature Selection & Extraction)

## 1) Conceptos clave
- **Selecci√≥n de caracter√≠sticas**: eliges un **subconjunto** de variables originales (no las transformas).  
- **Extracci√≥n de caracter√≠sticas**: **transformas** las variables para crear nuevas representaciones (ej. PCA, Autoencoders).

---

## 2) Mapa r√°pido de t√©cnicas (qu√© son y cu√°ndo usarlas)

| T√©cnica | Tipo | Idea central | Pros | Contras | √ösala cuando‚Ä¶ |
|---|---|---|---|---|---|
| **RFE (Recursive Feature Elimination)** | *Wrapper* | Entrena un modelo, elimina las caracter√≠sticas menos importantes y repite hasta quedarte con k | Buena para modelos lineales/√°rboles; subgrupo interpretable | Costosa si hay muchas features; depende del modelo base | Tienes un estimador con `coef_` o `feature_importances_` y quieres **selecci√≥n expl√≠cita** |
| **B√∫squeda exhaustiva (Best Subset)** | *Wrapper* | Prueba **todas** las combinaciones y elige la mejor seg√∫n CV/AIC/BIC | √ìptimo global (seg√∫n criterio) | **Explota** en tiempo: O(2^p) | P < ~25 y necesitas rigor m√°ximo |
| **Forward/Backward/Stepwise** | *Wrapper* | Agrega (forward) o elimina (backward) de a una feature | Mucho m√°s r√°pido que exhaustiva | Puede caer en √≥ptimos locales | Dataset mediano/grande; buscas buen trade-off tiempo/calidad |
| **Lasso (L1)** | *Embedded* | Penaliza |coef| y **lleva a cero** algunos ‚Üí selecci√≥n autom√°tica | Hace ‚Äúsparse‚Äù el modelo; bueno con muchas features | Inestable con colinealidad fuerte; necesita **escalar** | Quieres selecci√≥n + regularizaci√≥n en una sola pasada |
| **Ridge (L2)** | *Embedded* | Penaliza |coef| pero **no los hace cero** ‚Üí reduce varianza | Estable con colinealidad; mejora generalizaci√≥n | No hace selecci√≥n; solo encoge | Quieres **estabilidad** y evitar overfitting sin eliminar variables |
| **PCA** | *Extracci√≥n* | Componentes lineales que maximizan varianza | Reduce dimensi√≥n, quita colinealidad | Pierde interpretabilidad | Preparaci√≥n antes de modelos sensibles a colinealidad |
| **Autoencoders** | *Extracci√≥n* | Reducci√≥n no lineal aprendida | Capta patrones complejos | Menos interpretable; requiere m√°s datos | Estructuras no lineales/alta dimensi√≥n (im√°genes, se√±ales) |

---

## 3) RFE ‚Äî Recursive Feature Elimination
**Qu√© hace**: usa un modelo base (p.ej. `LinearRegression`, `LogisticRegression`, `RandomForest`) para puntuar features, elimina las peores y repite hasta llegar a `n_features_to_select`.

**Ventajas**: resultado interpretable (subset real), aprovecha conocimiento del modelo.  
**Cuidado**: *data leakage* ‚Üí haz RFE **dentro** de un `Pipeline` y valida con CV.

**Ejemplo (scikit-learn)**:
```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import RFE
from sklearn.linear_model import Lasso
from sklearn.model_selection import cross_val_score, KFold

pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('rfe', RFE(estimator=Lasso(alpha=0.01, max_iter=10000), n_features_to_select=10)),
    ('model', Lasso(alpha=0.01, max_iter=10000))
])

cv = KFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(pipe, X, y, cv=cv, scoring='neg_mean_squared_error')
print("MSE CV:", -scores.mean())

## **4**) B√∫squeda exhaustiva / Secuencial

Exhaustiva (Best Subset): eval√∫a todas las combinaciones. Ideal si P es peque√±o y buscas ‚Äúlo mejor‚Äù seg√∫n un criterio (AIC/BIC/CV).
Alternativa pr√°ctica: Forward/Backward/Stepwise con SequentialFeatureSelector (scikit-learn).

Ejemplo (Forward Selection con CV):



```
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.linear_model import Ridge
from sklearn.model_selection import TimeSeriesSplit  # si es serie temporal

est = Ridge(alpha=1.0)
sfs = SequentialFeatureSelector(
    est, n_features_to_select=12, direction='forward', scoring='neg_mean_squared_error', cv=5
)
sfs.fit(X, y)

mask = sfs.get_support()
selected_features = X.columns[mask]  # si X es DataFrame
print(selected_features)
```

## Gu√≠a de decisi√≥n r√°pida

Pocas features (P < 25) y quieres rigor ‚Üí Exhaustiva (o Stepwise bien hecho).

Muchas features y quieres subset interpretable ‚Üí RFE (con un buen estimador) o Lasso.

Colinealidad fuerte ‚Üí Ridge (estabilidad) o Elastic Net (si adem√°s quieres selecci√≥n).

Modelo final es √°rbol/boosting ‚Üí muchas veces no necesitas selecci√≥n expl√≠cita; usa feature importance y/o max_depth, min_child_weight, etc.

Series temporales ‚Üí usa TimeSeriesSplit, evita fuga temporal y no mezcles futuro en el escalado/selecci√≥n.

## Selecci√≥n vs Extracci√≥n

**RECETARIO**
"""

# Ejemplo: PCA+Lasso en pipeline con TimeSeriesSplit
from sklearn.decomposition import PCA
from sklearn.linear_model import LassoCV
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline # Import Pipeline
from sklearn.model_selection import TimeSeriesSplit # Import TimeSeriesSplit
import numpy as np # Import numpy for creating dummy data

# Define tscv
tscv = TimeSeriesSplit(n_splits=5)

# Create dummy data for X and y
X = np.random.rand(100, 10) # 100 samples, 10 features
y = np.random.rand(100) # 100 samples

pipe_pca_lasso = Pipeline([
    ('scaler', StandardScaler()),
    ('pca', PCA(n_components=0.95)),
    ('lasso', LassoCV(cv=tscv, n_jobs=-1, max_iter=10000))
])
pipe_pca_lasso.fit(X, y)
print("n comps PCA:", pipe_pca_lasso.named_steps['pca'].n_components_)
print("alpha* Lasso:", pipe_pca_lasso.named_steps['lasso'].alpha_)

import numpy as np
import matplotlib.pyplot as plt
import scipy.stats as st
import statsmodels.api as sm
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LassoCV
from sklearn.model_selection import TimeSeriesSplit

# Reusa tu pipe y tscv ya definidos
# pipe_pca_lasso = Pipeline([...])
# tscv = TimeSeriesSplit(n_splits=5)

# Toma la √∫ltima partici√≥n temporal como ‚Äúvalidaci√≥n final‚Äù
splits = list(tscv.split(X))
train_idx, val_idx = splits[-1]

pipe_pca_lasso.fit(X[train_idx], y[train_idx])
y_val_pred = pipe_pca_lasso.predict(X[val_idx])

def diag_plot(y_true, y_pred, title="Diagn√≥stico"):
    resid = y_true - y_pred
    fig, ax = plt.subplots(1, 3, figsize=(15, 4))
    ax[0].scatter(y_true, y_pred, s=10)
    ax[0].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'k--')
    ax[0].set_title("Real vs Pred")
    ax[1].hist(resid, bins=30, density=True)
    ax[1].set_title("Hist residuales")
    sm.qqplot(resid, line='s', ax=ax[2])
    ax[2].set_title("Q-Q residuales")
    plt.suptitle(title)
    plt.tight_layout()
    plt.show()

diag_plot(y[val_idx], y_val_pred, title="Diagn√≥stico (TimeSeriesSplit OOS)")

"""## 1. Real vs Pred (izquierda)

**Qu√© muestra:** los valores reales (y_true) en el eje X contra las predicciones (y_pred) en el eje Y.

La l√≠nea negra es la bisectriz (y = x), donde caer√≠an los puntos si el modelo predijera perfecto.

**Interpretaci√≥n del gr√°fico:**

Tus predicciones est√°n casi todas alrededor de un valor constante ‚Üí parece que el modelo no est√° capturando la variabilidad real.

**En proyecto:** aqu√≠ se√±alas si el modelo ‚Äúse ajusta bien‚Äù o si tiende a subestimar/sobreestimar.

## 2. Histograma de residuales (centro)

**Qu√© muestra:** distribuci√≥n de los errores (resid = y_true - y_pred).

**Lo ideal:** que sea sim√©trica, centrada en 0, sin sesgos fuertes.

**Interpretaci√≥n del gr√°fico:**

Los residuales no parecen centrados exactamente en 0 (se concentran entre -0.4 y -0.2).

Eso indica un sesgo sistem√°tico: el modelo tiende a equivocarse hacia un lado.

## 3. Q-Q plot de residuales (derecha)

**Qu√© muestra:** compara la distribuci√≥n de los residuales contra una normal te√≥rica.

**Lo ideal:** los puntos deber√≠an caer cerca de la l√≠nea roja ‚Üí significa que los errores son aproximadamente normales.

**Interpretaci√≥n del gr√°fico:**

**Hay curvatura clara: **los residuales no siguen bien una normal.

Esto sugiere que el supuesto de normalidad no se cumple (algo esperable si el modelo no est√° ajustando bien).

# üìå ¬øQu√© es PCA + Lasso en pipeline?

### 1. PCA (Principal Component Analysis)
- **Qu√© hace**: reduce la dimensionalidad, condensando la informaci√≥n de muchas variables en unas pocas **componentes principales** que capturan la mayor parte de la varianza.  
- **Para qu√© sirve**:
  - Quita colinealidad entre variables.
  - Reduce ruido.
  - Acelera el entrenamiento.

### 2. Lasso (L1 Regularization)
- **Qu√© hace**: es una regresi√≥n penalizada que fuerza algunos coeficientes a **cero** ‚Üí act√∫a como **selecci√≥n autom√°tica de caracter√≠sticas**.  
- **Para qu√© sirve**:
  - Evita sobreajuste (overfitting).
  - Identifica qu√© variables son realmente importantes.
  - Hace que el modelo sea m√°s interpretable.

### 3. El pipeline completo
```python
pipe_pca_lasso = Pipeline([
    ('scaler', StandardScaler()),       # estandariza (necesario para PCA y Lasso)
    ('pca', PCA(n_components=0.95)),    # conserva 95% de la varianza
    ('lasso', LassoCV(cv=tscv))         # selecciona alpha √≥ptimo y elimina ruido
])

## Flujo:
Datos crudos ‚Üí escalado ‚Üí reducci√≥n de dimensionalidad (PCA) ‚Üí selecci√≥n y regularizaci√≥n (Lasso).

**Validaci√≥n:** usar TimeSeriesSplit, que es lo correcto en series temporales (no mezclas futuro en entrenamiento).

‚úÖ **¬øPor qu√© es una ‚Äúreceta ganadora‚Äù?**

Porque combina extracci√≥n (PCA) y selecci√≥n (Lasso).

Quita redundancia y ruido antes de entrenar.

Se adapta bien a datasets con muchas variables correlacionadas y series temporales.

El alpha* que devuelve Lasso y el n_components que devuelve PCA son interpretables: puedes decir cu√°ntas dimensiones y qu√© nivel de regularizaci√≥n resultaron √≥ptimos.

# üç≤ Recetas ganadoras para predicci√≥n / forecast en series temporales

## 1) Feature Engineering + Regularizaci√≥n
- **Receta:** crear **features rezagadas (lags)**, medias m√≥viles, diferencias y variables de calendario (hora, d√≠a, mes, festivo, etc.).  
- **Modelo:** Ridge / Lasso / ElasticNet.  
- **Cu√°ndo usarla:** cuando hay mucha correlaci√≥n temporal pero tambi√©n estacionalidad.  
- **Extra:** usar `TimeSeriesSplit` o `WalkForwardValidation`.

---

## 2) RFE + √Årboles (RandomForest o Gradient Boosting)
- **Receta:** usar RFE (Recursive Feature Elimination) o feature importance de √°rboles para reducir features ‚Üí entrenar con **XGBoost / LightGBM / CatBoost**.  
- **Cu√°ndo usarla:** datasets grandes con muchas variables ex√≥genas (clima, econom√≠a, sensores).  
- **Ganancia:** √°rboles capturan relaciones no lineales y manejan outliers sin problema.

---

## 3) PCA/Autoencoders + Red Neuronal
- **Receta:** compresi√≥n de alta dimensi√≥n (PCA lineal o Autoencoder no lineal) ‚Üí luego LSTM, GRU o Transformer.  
- **Cu√°ndo usarla:** cuando hay much√≠simas variables correlacionadas (ej. datos energ√©ticos, financieros, sensores IoT).  
- **Ganancia:** menos ruido y entrenamiento m√°s estable.

---

## 4) Ensembling (Combinaci√≥n de modelos)
- **Receta:** combinar modelos lineales + √°rboles + redes.  
  Ejemplo: promedio ponderado de ARIMA, XGBoost y LSTM.  
- **Cu√°ndo usarla:** cuando un solo modelo no domina; √∫til en competiciones Kaggle.  
- **Ganancia:** reduce varianza, mejora robustez.

---

## 5) Modelos cl√°sicos + ML h√≠brido
- **Receta:** ajustar un ARIMA / ETS para capturar tendencia y estacionalidad ‚Üí usar ML (XGBoost, NN) sobre los **residuales**.  
- **Cu√°ndo usarla:** cuando la serie tiene patr√≥n cl√°sico + se√±ales no lineales en los residuales.  
- **Ganancia:** cada parte del modelo ataca un componente distinto.

---

## 6) Clustering + Forecast
- **Receta:** agrupar series similares (ej. k-means sobre perfiles normalizados) ‚Üí entrenar un modelo por cl√∫ster.  
- **Cu√°ndo usarla:** paneles de series (ej. m√∫ltiples clientes, estaciones meteorol√≥gicas).  
- **Ganancia:** modelos m√°s espec√≠ficos y precisos.

---

## 7) Deep Learning puro
- **Opciones:**  
  - **LSTM / GRU:** para dependencias largas.  
  - **Temporal Convolutional Networks (TCN):** capturan dependencias temporales con convoluciones.  
  - **Transformers (Attention):** muy potentes para multivariadas largas.  
- **Cu√°ndo usarla:** gran cantidad de datos, relaciones no lineales y dependencias largas.  
- **Ganancia:** precisi√≥n SOTA en forecasting complejo.

---

## 8) Stacking con metamodelo
- **Receta:** entrenar varios modelos base (ARIMA, XGB, LSTM) ‚Üí sus predicciones se usan como input de un **metamodelo** (ej. Ridge o NN peque√±a).  
- **Cu√°ndo usarla:** datasets ricos y tiempo de c√≥mputo disponible.  
- **Ganancia:** saca lo mejor de cada modelo.

# üîé Estrategias para predicci√≥n y forecast ‚Äî Comparativa

| Estrategia | Descripci√≥n | Ventajas | Desventajas | Cu√°ndo usarla |
|------------|-------------|----------|-------------|---------------|
| **Feature Engineering + Regularizaci√≥n (Ridge/Lasso/ElasticNet)** | Crear lags, medias m√≥viles, estacionalidad y ajustar un modelo lineal regularizado. | Simple, interpretable, evita sobreajuste, r√°pido. | Lineal, puede no capturar relaciones no lineales complejas. | Series peque√±as/medianas con patrones claros y variables ex√≥genas. |
| **RFE + √Årboles (RandomForest, XGBoost, LightGBM, CatBoost)** | Selecci√≥n de variables (RFE o feature importance) y luego boosting/bagging. | Maneja no linealidad, robusto a outliers, funciona con muchas features. | M√°s costoso en c√≥mputo, tuning necesario. | Datos multivariados grandes (clima, energ√≠a, econom√≠a). |
| **PCA/Autoencoder + LSTM/GRU** | Reducir dimensionalidad con PCA/Autoencoder y alimentar a una red recurrente. | Quita ruido y colinealidad, capta dependencias largas. | Menos interpretable, requiere muchos datos. | Forecast con muchas variables correlacionadas (IoT, sensores). |
| **Ensembling (promedio/ponderado)** | Combinar varios modelos y promediar sus predicciones. | Reduce varianza, mejora robustez, simple de implementar. | No siempre mejora si los modelos son muy parecidos. | Competencias, entornos donde la estabilidad es clave. |
| **H√≠brido ARIMA + ML en residuales** | ARIMA para tendencia/estacionalidad, ML para residuales. | Aprovecha fortalezas de ambos mundos, interpretable. | M√°s pasos, tuning doble, posible sobreajuste. | Series con estructura cl√°sica + se√±ales no lineales. |
| **Clustering + Forecast** | Agrupar series similares (ej. k-means) y entrenar por cl√∫ster. | Modelos m√°s espec√≠ficos y precisos, reduce varianza. | Requiere definir n√∫mero de cl√∫steres, puede fragmentar demasiado. | Paneles de series (ej. clientes, estaciones meteorol√≥gicas). |
| **Deep Learning (LSTM, GRU, TCN, Transformers)** | Redes recurrentes, convolucionales o basadas en atenci√≥n. | Capturan dependencias largas y no lineales, estado del arte. | Alto costo de entrenamiento, poca interpretabilidad. | Datasets grandes y complejos, forecasting multivariado. |
| **Stacking con metamodelo** | Entrenar varios modelos base y combinar con un metamodelo (ej. Ridge). | Saca lo mejor de cada modelo, flexible. | Complejo de implementar, costoso. | Forecast con m√∫ltiples fuentes de datos y tiempo de c√≥mputo suficiente. |
"""

# Instalar graphviz (si no est√°)
!apt-get -qq install graphviz
!pip install graphviz

from graphviz import Digraph

dot = Digraph(comment="Receta de Estrategia Forecast")
dot.attr(rankdir='TB', size='8')

# Nodos principales
dot.node('A', '¬øCantidad de datos disponibles?')
dot.node('B', 'Pocos (<1000 obs)\n‚Üí Modelos cl√°sicos')
dot.node('C', 'Muchos (>1000 obs)\n‚Üí Modelos avanzados')

dot.edge('A', 'B', label='Pocos')
dot.edge('A', 'C', label='Muchos')

# Rama Modelos cl√°sicos
dot.node('B1', 'ARIMA/ETS\n(patr√≥n lineal y estacionalidad)')
dot.node('B2', 'Lasso/Ridge\n(si hay variables ex√≥genas)')
dot.edge('B', 'B1')
dot.edge('B', 'B2')

# Rama Modelos avanzados
dot.node('D', '¬øMuchas variables ex√≥genas correlacionadas?')
dot.edge('C', 'D')

dot.node('E', 'PCA o Autoencoder + LSTM/GRU')
dot.node('F', '¬øRelaciones no lineales fuertes?')
dot.edge('D', 'E', label='S√≠')
dot.edge('D', 'F', label='No')

dot.node('G', 'XGBoost / LightGBM / CatBoost')
dot.node('H', 'Ridge/Lasso con\nlags y medias m√≥viles')
dot.edge('F', 'G', label='S√≠')
dot.edge('F', 'H', label='No')

dot.node('I', '¬øQuieres a√∫n m√°s precisi√≥n?')
dot.edge('G', 'I')

dot.node('J', 'Stacking / Ensembling')
dot.edge('I', 'J', label='S√≠')

# Rama Deep Learning largo
dot.node('K', '¬øSerie muy larga y compleja?')
dot.edge('E', 'K')

dot.node('L', 'Transformers / TCN')
dot.node('M', 'LSTM / GRU')
dot.edge('K', 'L', label='S√≠')
dot.edge('K', 'M', label='No')

# Nodo final com√∫n
dot.node('Z', 'Diagn√≥stico de residuales\n(Asegurar buen ajuste)')
for n in ['B1','B2','H','M','L','J']:
    dot.edge(n, 'Z')

# Mostrar en colab
dot.render('forecast_recipe.gv', view=True)
dot